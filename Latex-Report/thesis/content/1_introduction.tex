\chapter{Introduction}
\label{ch1_introduction}
\section{Motivation}
\label{sect1_1}
Moore’s legendary law: "The number of transistors and resistors on a chip doubles every 18 months," which predicts the pace of technology scaling is largely mistranslated to imply CPU performance. Although conventional scaling techniques have challenged the tacit promises of performance put forth by Moore, Intel - co-founded by Moore himself - has found novel ways to steadily stride along his prognosis, an achievement that can be attributed to a legion of engineers. However, as we approached infinitesimally small-sized transistors, we chalked up paltry performance gains and came to terms with the fact that purely upgrading hardware generations is not the solution. 
\newline \newline
The Figure \ref{fig:sematech} illustrates the gap between the computational demand with increasing complexity and the actual productivity. Increasing the operating frequency or the number of cores does not yield the performance desired from the current complex, compute-intensive applications. \newline \newline
\begin{figure}[h!]
  \includegraphics[width=\linewidth]{figures/sematech.jpg}
  \caption{The Productivity Gap
  \cite{industry1999international}}
  \label{fig:sematech}
\end{figure}
This calls for multi-core systems that achieve the desired performance by integrating specialized processing abilities required for specific tasks. Heterogeneous system architectures exploit multiple processor types, to realize the best of both data-parallel and task-parallel (sequential) applications \cite{amd_resources}.\newline \newline
To realize the full potential of such systems, the system designers should scrupulously integrate the diverse compute elements available in the platform and allow them to work together seamlessly. While conventional accelerators limit productivity by demanding high skills in hardware engineering, heterogeneous architectures having GPPs and specialized co-processors offer software-like programmability, enhanced application portability and consequently, improved productivity. Some of the popular heterogeneous computing platforms include network processors such as Intel IXP, Embedded systems such as TI OMAP, NVIDIA Tegra and Apple A9, Reconfigurable devices such as Xilinx FPGAs (Virtex, Kintex, etc.) in Zynq platform containing dual core ARM Cortex A9 Processors, General purpose processors such as IBM Cell and ARM big.LITTLE CPU architectures \cite{wiki_nanoscale}. \newline \newline
These devices guarantee a power-aware design with increased data parallelism and throughput. However, we should also be mindful of the challenges they pose such as different instruction set architectures for different compute elements, varying cache structure and coherence protocols, varying memory access patterns (uniform or non-uniform) and interface types \cite{wiki_nanoscale}, etc. Opaque programming paradigms (like POSIX standard for threads), the differences in underlying micro-architecture and abstractions associated with high-level language programming can impede performance predictions and sometimes, increase power consumption. Designers should explicitly handle thread synchronization and shared variable protection in multi-threaded applications and partition the application suitably between various computing elements. Example: Running a sequential task on FPGA leads to under-utilization of resources and slows down the performance. Similarly, performing SIMD operations on a CPU would be a bad choice. Design decisions generally involve domain expertise and design space exploration, which is a quantitative approach to recognizing the design variables with the most beneficial effect on the system’s performance goals. \newline \newline
To mitigate the challenges listed above, we need to establish a standard programming model that is portable across devices and capable of delivering the desired performance. 
For simple applications, design decisions are straightforward. This prompts us to explore some complex, compute-intensive applications and study the impact of various design decisions on their performance. This report intends to investigate two such applications, namely MNIST Digit classifier using Convolutional Neural Network, and Fully Homomorphic Encryption scheme, and to optimize them for best-case acceleration on a heterogeneous platform. 

\section{Contribution}
\label{sect1_2}
The primary focus areas of this thesis can be summarized as follows:
\begin{itemize}
\item Identification and understanding of two applications of high computational complexity which show potential for hardware acceleration.
\item Understanding of programming models best suited for the parallelization of the identified complex applications.
\item Profiling various parts of the applications to isolate the critical paths that need improvement.
\item Performing architectural exploration and suitably partitioning the application between various compute elements available in the platform.
\item Running and profiling the applications to study the performance improvements with the modified design.
\end{itemize}
\section{Organization}
\label{sect1_3}
The report consists of the following chapters: 
\textbf{Chapter \ref{ch2_background}} presents background knowledge, software and hardware requirements needed for this thesis. 
\textbf{Chapter \ref{ch3_cnn}} delves into the C++ and OpenCL implementation of MNIST Digit Recognition program and studies the runtime benefits with parallel execution.
\textbf{Chapter \ref{ch4_fhew}} explains another complex encryption algorithm implemented in software that exhibits potential for hardware acceleration.
\textbf{Chapter \ref{ch5_conclusion}} draws conclusion to the contents perused in this thesis and throws light on potential direction for future research.