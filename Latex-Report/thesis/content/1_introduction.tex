\chapter{Introduction}
\label{ch1_introduction}
\section{Motivation}
\label{sect1_1}
Moore’s legendary law: "The number of transistors and resistors on a chip doubles every 18 months," which predicts the pace of technology scaling is largely mistranslated to imply CPU performance. Although conventional scaling techniques have challenged the tacit promises of performance put forth by Moore, Intel - co-founded by Moore himself - has found novel ways to steadily stride along his prognosis, an achievement that can be attributed to a legion of engineers. However, as we approached infinitesimally small-sized transistors, we chalked up paltry performance gains and came to terms with the fact that purely upgrading hardware generations is not the solution. 
\newline \newline
The Figure \ref{fig:sematech} illustrates the gap between the computational demand with increasing complexity and the actual productivity. Increasing the operating frequency or the number of cores does not yield the performance desired from the current complex, compute-intensive applications. \newline \newline
\begin{figure}[h!]
  \includegraphics[width=\linewidth]{figures/sematech.jpg}
  \caption{The Productivity Gap
  \cite{industry1999international}}
  \label{fig:sematech}
\end{figure}
Heterogeneous system architectures exploit multiple processor types, to realize the best of both data-parallel and task-parallel (sequential) applications \cite{amd_resources}. To realize the full potential of such systems, the system designers should scrupulously integrate the diverse compute elements available in the platform and allow them to work together seamlessly. While conventional accelerators limit productivity by demanding high skills in hardware engineering, heterogeneous architectures having GPPs and specialized co-processors offer software-like programmability, enhanced application portability and consequently, improved productivity. Some of the popular heterogeneous computing platforms include network processors such as Intel IXP, Embedded systems such as TI OMAP, NVIDIA Tegra and Apple A9, Reconfigurable devices such as Xilinx FPGAs (Virtex, Kintex, etc.) in Zynq platform containing dual core ARM Cortex A9 Processors, General purpose processors such as IBM Cell and ARM big.LITTLE CPU architectures \cite{wiki_nanoscale}. \newline \newline 
These devices guarantee a power-aware design with increased data parallelism and throughput. However, we should also be mindful of the challenges they pose such as different instruction set architectures for different compute elements, varying cache structure and coherence protocols, varying memory access patterns (uniform or non-uniform) and interface types \cite{wiki_nanoscale}, etc. Opaque programming paradigms (like POSIX standard for threads), the differences in underlying micro-architecture and abstractions associated with high-level language programming can impede performance predictions and sometimes, increase power consumption. Designers should explicitly handle thread synchronization and shared variable protection in multi-threaded applications and partition the application suitably between various computing elements. Example: Running a sequential task on FPGA leads to under-utilization of resources and slows down the performance. Similarly, performing SIMD operations on a CPU would be a bad choice. Design decisions generally involve domain expertise and design space exploration, which is a quantitative approach to recognizing the design variables with the most beneficial effect on the system’s performance goals. \newline \newline
To mitigate the challenges listed above, we need to establish a standard programming model that is portable across devices and capable of delivering the desired performance. 
For simple applications, design decisions are straightforward. This prompts us to explore some complex, compute-intensive applications and study the impact of various design decisions on their performance. 
This report deals with the study of two such compute-intensive applications, namely MNIST Digit classifier using Convolutional Neural Network (CNN), and Fully Homomorphic Encryption (FHE), their efficient partitioning between hardware and software and gauging of various design points to achieve a specific optimization goal, i.e. low latency. For example, the first convolutional layer of the MNIST-CNN having a computation demand of 300K multiply-accumulate operations can be processed at 24M MACs/s with single-threaded flow on Intel Core i3 platform, whereas the same platform can process upto 2B MACs/s with multi-threaded OpenCL execution. To achieve sub-microsecond latency, 20 convolution engines (running at 600 MHz) can be used where each engine is able to produce one pixel by performing 25 MAC operations every clock cycle. The peak performance of such a hypothetical engine is 300 billion MAC/sec. In a broader sense, a mere 14 frames/s with sequential flow can be boosted to about 826 frames/s on the same CPU while a cross-platform performance of as high as 3105 frames/s can be accomplished with parallel thread executions. We also present similar analysis for second application (FHE). Here, we focus mainly on the algorithmic design exploration for a specific architecture using high-level synthesis tools.


\section{Contribution}
\label{sect1_2}
The primary focus areas of this thesis can be summarized as follows:
\begin{itemize}
\item Identification and understanding of two applications with high computational complexity which show potential for hardware acceleration.
\item Understanding of programming models best suited for the parallelization of the identified complex applications.
\item Profiling various parts of the applications to isolate the critical paths that need improvement.
\item Performing architectural exploration and suitably partitioning the application between various compute elements available in the platform.
%\item Running and profiling the applications to study the performance improvements with the modified design.
\end{itemize}

\section{Organization}
\label{sect1_3}
The report consists of the following chapters: 
\textbf{Chapter \ref{ch2_background}} presents background knowledge, software and hardware requirements needed for this thesis. 
\textbf{Chapter \ref{ch3_cnn}} delves into the C++ and OpenCL implementation of MNIST Digit Recognition program and studies the runtime benefits with parallel execution.
\textbf{Chapter \ref{ch4_fhew}} explains another complex encryption algorithm implemented in software that exhibits potential for hardware acceleration.
\textbf{Chapter \ref{ch5_conclusion}} draws conclusion to the contents perused in this thesis and throws light on potential direction for future research.