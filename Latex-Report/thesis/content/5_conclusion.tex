\chapter{Conclusion and Future Work}
\label{ch5_conclusion}
\section{Conclusion}
\label{6_1}
This report discussed hardware acceleration using heterogeneous architectures, specifically GPUs and FPGAs by means of threaded programming (OpenCL) and high level synthesis. As the optimization objective was improving execution speed, several compiler and platform-specific optimizations were used to inspect the gain in runtime. This report covered the study of two trending compute-intensive applications, isolation of software hot-spots, design decisions and improved implementation using heterogeneous hardware platforms. \newline \newline Our experiments with MNIST digit classifier revealed that when the sequential C++ code was translated to parallel threads spawned together for concurrent execution, the speed-up was massive. For example, the CNN code running on Intel Core i3-2350M platform took around 70.029 ms to classify a single image when executing sequentially, while it took only 1.2 ms with multi-threaded OpenCL code. The number of multiply-accumulate operations performed per second increased from 24M MACs/s to 1B MACs/s for convolution layer 1 on the same platform. This layer executed almost 57 times faster with the OpenCL code \ref{table:sw_hotspots}\ref{table:conv1_inference}. Also, a cross-platform speedup of as high as 139 times \ref{table:conv1_inference} was achieved with the Intel Xeon ES-1650 CPU. This speedup was almost comparable to the gain that can be accomplished using a hypothetical platform with 20 RISC processors running at 100 MHz each and processing 1 MAC/cycle each. A huge dataset comprising of 10000 images was classified in a few microseconds. It also revealed that any OpenCL compliant device, CPU or GPU, can offer acceleration depending on the number of low power cores available in the platform and also the memory access model. Hence, understanding of all the OpenCL models is crucial to schedule the work suitably among different compute units. The study of Fully Homomorphic Encryption scheme revealed that domain expertise is also a key factor to achieve hardware acceleration, in addition to accelerator-awareness. All computations performed in FHE are on ring lattices and hence, offloading the entire Bootstrapping logic onto hardware requires sound mathematical background on lattice-based computations. We noticed a speedup of 126 times with MachSuite benchmark running on hardware compared to the handwritten FFT executing in software. The key challenge in hardware acceleration is ensuring that functionality is preserved upon offloading to an accelerator. 
\section{Future Work}
Future works could incorporate:
\begin{itemize}
\item \textbf{Runtime analysis on coarse-grained and fine-grained Overlay Architectures} with efficient interfacing between host processor, DSP Units and other high-speed vector engines.
\item Identifying a novel way to \textbf{fit in a single AddToAccumulator block onto a single FPGA}: \\The challenge in doing so is the huge dimensionality involved (Section \ref{4_1_3_1}). Computations on ciphertexts take up alot of area and hence, these dimensions have to be intuitively handled.
\item \textbf{Reuse of the hardware FFT blocks in the new TFHE Library} implemented in April, 2017, to verify the generality of the implementation. As TFHE library already promises bootstrapping speed of less than 0.1 seconds, the gain in hardware for bootstrapping can be studied by porting specific "hot" functions to the hardware.
\end{itemize}